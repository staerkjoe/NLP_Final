{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"13dD_czvNiYiFJ75bb1_eUl_aADHeTTUM","authorship_tag":"ABX9TyMumIW1A/HvTVfylf8niORC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Imports**"],"metadata":{"id":"GWIBhZmMzoZG"}},{"cell_type":"code","source":["import re\n","\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","import spacy\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import (\n","    accuracy_score,\n","    classification_report,\n","    confusion_matrix,\n","    ConfusionMatrixDisplay\n",")"],"metadata":{"id":"v24vl1vTy2ey","executionInfo":{"status":"ok","timestamp":1748501257664,"user_tz":-120,"elapsed":8652,"user":{"displayName":"Johannes Stärk","userId":"02868779539657266620"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["**Load and preprocess data**"],"metadata":{"id":"R2nVn0P7wvVK"}},{"cell_type":"code","source":["# Load Data\n","df = pd.read_parquet('/content/drive/MyDrive/NLP News Bias Data/data_newsbias_cleaned.parquet', columns=['text', 'bias_label'])\n","df = df.dropna(subset=['text', 'bias_label']).copy()\n","\n","# Lowercase, remove punctuation, remove stopwords\n","def clean_for_model(text):\n","    text = text.lower()\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    tokens = text.split()\n","    tokens = [t for t in tokens if t not in ENGLISH_STOP_WORDS]\n","    return ' '.join(tokens)\n","\n","# Apply cleaning\n","df['text_clean'] = df['text'].astype(str).apply(clean_for_model)\n","\n","# Keep only cleaned text + label\n","df_cleaned = df[['text_clean', 'bias_label']]\n","\n","# Save to compact Parquet\n","df_cleaned.to_parquet('/content/drive/MyDrive/NLP News Bias Data/df_cleaned_model_ready.parquet', compression='snappy')\n","\n","print(f\"Cleaned data saved: {df_cleaned.shape[0]} rows\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"HIIBU5z9cON_","executionInfo":{"status":"error","timestamp":1748501996097,"user_tz":-120,"elapsed":732328,"user":{"displayName":"Johannes Stärk","userId":"02868779539657266620"}},"outputId":"3d9b90d9-a0c4-44a1-e957-e293d66ada3d"},"execution_count":2,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b69efa3da8a2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Save to compact Parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/NLP News Bias Data/df_cleaned_model_ready.parquet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'snappy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cleaned data saved: {df_cleaned.shape[0]} rows\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 )\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   3111\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3113\u001b[0;31m         return to_parquet(\n\u001b[0m\u001b[1;32m   3114\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3115\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFilePath\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mWriteBuffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m     impl.write(\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;31m# write to single output file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 self.api.parquet.write_table(\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36mwrite_table\u001b[0;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, column_encoding, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, write_page_index, write_page_checksum, sorting_columns, store_decimal_as_integer, **kwargs)\u001b[0m\n\u001b[1;32m   1926\u001b[0m                 \u001b[0mstore_decimal_as_integer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstore_decimal_as_integer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m                 **kwargs) as writer:\n\u001b[0;32m-> 1928\u001b[0;31m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_group_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow_group_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1929\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_path_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36mwrite_table\u001b[0;34m(self, table, row_group_size)\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_group_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow_group_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["**Baselines: Apply Naive Bayes and Logisitc Regression with BoW and TF-IDF**"],"metadata":{"id":"V1_rI7Guw_hA"}},{"cell_type":"code","source":["# Load cleaned dataset\n","df = pd.read_parquet('/content/drive/MyDrive/NLP News Bias Data/df_cleaned_model_ready.parquet')\n","\n","# Randomly sample 50k rows per class (balanced total = 150k)\n","df = (\n","    df.groupby('bias_label', group_keys=False)\n","      .apply(lambda g: g.sample(50000, random_state=42))\n","      .reset_index(drop=True)\n",")\n","\n","# Features and labels\n","X = df['text_clean']\n","y = df['bias_label']\n","\n","# Train/test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, stratify=y, test_size=0.2, random_state=42\n",")\n","\n","# Vectorizers\n","vectorizers = {\n","    'BoW': CountVectorizer(stop_words='english', max_features=20000),\n","    'TF-IDF': TfidfVectorizer(stop_words='english', max_features=20000)\n","}\n","\n","# Models\n","models = {\n","    'LogisticRegression': LogisticRegression(max_iter=500, solver='saga'),\n","    'NaiveBayes': MultinomialNB()\n","}\n","\n","# Run all combinations\n","for vec_name, vectorizer in vectorizers.items():\n","    print(f\"\\n🔹 Vectorizing with {vec_name}...\")\n","    X_train_vec = vectorizer.fit_transform(X_train)\n","    X_test_vec = vectorizer.transform(X_test)\n","\n","    for model_name, model in models.items():\n","        print(f\"\\n{model_name} + {vec_name}\")\n","        model.fit(X_train_vec, y_train)\n","        preds = model.predict(X_test_vec)\n","        acc = accuracy_score(y_test, preds)\n","\n","        print(f\"Accuracy: {acc:.4f}\")\n","        print(classification_report(y_test, preds))\n","\n","        # Confusion matrix\n","        cm = confusion_matrix(y_test, preds, labels=model.classes_)\n","        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n","        disp.plot(cmap='Blues', values_format='d')\n","        plt.title(f\"{model_name} + {vec_name} Confusion Matrix\")\n","        plt.show()"],"metadata":{"id":"eLptXAsEzVVi","executionInfo":{"status":"aborted","timestamp":1748501996105,"user_tz":-120,"elapsed":19,"user":{"displayName":"Johannes Stärk","userId":"02868779539657266620"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Distribution of Sources of misclassified articles (LR with Bag of Words)**"],"metadata":{"id":"yZTik-RFw2qA"}},{"cell_type":"code","source":["# Load cleaned dataset\n","df = pd.read_parquet('/content/drive/MyDrive/NLP News Bias Data/df_sampled_cleaned.parquet')\n","\n","# Features and labels\n","X = df['text_clean']\n","y = df['bias_label']\n","\n","# Split the dataset\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, stratify=y, test_size=0.2, random_state=42\n",")\n","\n","# Vectorize with Bag of Words\n","vectorizer = CountVectorizer(stop_words='english', max_features=20000)\n","X_train_vec = vectorizer.fit_transform(X_train)\n","X_test_vec = vectorizer.transform(X_test)\n","\n","# Train Logistic Regression\n","lr_model = LogisticRegression(max_iter=500, solver='saga')\n","lr_model.fit(X_train_vec, y_train)\n","\n","# Predict and evaluate\n","y_pred = lr_model.predict(X_test_vec)\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(classification_report(y_test, y_pred))\n","\n","# Attach predictions to test data\n","df_test = df.iloc[y_test.index].copy()\n","df_test['predicted'] = y_pred\n","\n","# Identify misclassified samples\n","df_misclassified = df_test[df_test['bias_label'] != df_test['predicted']].copy()\n","\n","# Load spaCy for POS tagging\n","nlp = spacy.load(\"en_core_web_sm\")\n","tqdm.pandas()\n","\n","# Count NOUN and ADJ\n","def count_pos(text):\n","    doc = nlp(str(text))\n","    noun_count = sum(1 for token in doc if token.pos_ == \"NOUN\")\n","    adj_count = sum(1 for token in doc if token.pos_ == \"ADJ\")\n","    return pd.Series([noun_count, adj_count])\n","\n","df_misclassified[['noun_count', 'adj_count']] = df_misclassified['text_clean'].progress_apply(count_pos)\n","\n","# Add text length\n","df_misclassified['text_length'] = df_misclassified['text_clean'].apply(lambda x: len(str(x).split()))\n","\n","# Analysis\n","print(\"\\nTop 10 misclassified sources:\")\n","print(df_misclassified['source'].value_counts().head(10))\n","\n","print(\"\\nAverage noun count:\", df_misclassified['noun_count'].mean())\n","print(\"Average adjective count:\", df_misclassified['adj_count'].mean())\n","print(\"Average article length (in words):\", df_misclassified['text_length'].mean())\n"],"metadata":{"id":"NGaYANmI6pGc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Distributions of nouns, adjectives and text lengths across misclassified articles grouped by bias label**"],"metadata":{"id":"ovDWwvpgxj8R"}},{"cell_type":"code","source":["# Group misclassified samples by true bias label\n","grouped_by_label = df_misclassified.groupby('bias_label')[['noun_count', 'adj_count', 'text_length']].mean()\n","\n","# Display results\n","print(grouped_by_label)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_khUKGSMA5BB","executionInfo":{"status":"ok","timestamp":1748437801697,"user_tz":-120,"elapsed":12,"user":{"displayName":"Johannes Stärk","userId":"02868779539657266620"}},"outputId":"f4443b57-380b-4748-ae19-b19995ed803d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","📊 Average POS statistics for misclassified articles, grouped by true label:\n","            noun_count  adj_count  text_length\n","bias_label                                    \n","center      112.418612  31.541073   270.414065\n","left        126.301767  34.707903   306.753918\n","right       119.375149  33.351367   289.372176\n"]}]},{"cell_type":"markdown","source":["**Distributions of nouns, adjectives and text lengths across correctly classified articles grouped by bias label**"],"metadata":{"id":"DzFs49CAyVI5"}},{"cell_type":"code","source":["# Load the data and predictions\n","df = pd.read_parquet('/content/drive/MyDrive/NLP News Bias Data/df_sampled_cleaned.parquet')\n","\n","# Ensure the test set from previous run (same train/test split!)\n","X = df['text_clean']\n","y = df['bias_label']\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, stratify=y, test_size=0.2, random_state=42\n",")\n","\n","# Vectorize again to match original predictions\n","vectorizer = CountVectorizer(stop_words='english', max_features=20000)\n","X_train_vec = vectorizer.fit_transform(X_train)\n","X_test_vec = vectorizer.transform(X_test)\n","\n","# Re-train logistic regression (same as before)\n","lr_model = LogisticRegression(max_iter=500, solver='saga')\n","lr_model.fit(X_train_vec, y_train)\n","y_pred = lr_model.predict(X_test_vec)\n","\n","# Attach predictions to df\n","df_test = df.iloc[y_test.index].copy()\n","df_test['predicted'] = y_pred\n","\n","# Filter correctly classified rows\n","df_correct = df_test[df_test['bias_label'] == df_test['predicted']].copy()\n","\n","# Load spaCy\n","nlp = spacy.load(\"en_core_web_sm\")\n","tqdm.pandas()\n","\n","# POS tagging function\n","def count_pos(text):\n","    doc = nlp(str(text))\n","    noun_count = sum(1 for token in doc if token.pos_ == \"NOUN\")\n","    adj_count = sum(1 for token in doc if token.pos_ == \"ADJ\")\n","    return pd.Series([noun_count, adj_count])\n","\n","# Apply POS tagging and compute length\n","df_correct[['noun_count', 'adj_count']] = df_correct['text_clean'].progress_apply(count_pos)\n","df_correct['text_length'] = df_correct['text_clean'].apply(lambda x: len(str(x).split()))\n","\n","# Group by label and calculate averages\n","grouped_correct = df_correct.groupby('bias_label')[['noun_count', 'adj_count', 'text_length']].mean()\n","print(grouped_correct)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OatCvdBiKUA9","executionInfo":{"status":"ok","timestamp":1748441890656,"user_tz":-120,"elapsed":1596230,"user":{"displayName":"Johannes Stärk","userId":"02868779539657266620"}},"outputId":"c4c71494-1f8f-4bab-b5fb-f816ce502949"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n","100%|██████████| 20342/20342 [18:39<00:00, 18.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["            noun_count  adj_count  text_length\n","bias_label                                    \n","center      127.685467  33.146822   298.602208\n","left        166.153055  50.621074   396.582667\n","right       133.164256  37.373418   339.279687\n"]}]},{"cell_type":"markdown","source":["**Average length of all articles grouped by bias label**"],"metadata":{"id":"hF_UKEH8yBNG"}},{"cell_type":"code","source":["# Compute text length (in words)\n","df['text_length'] = df['text_clean'].apply(lambda x: len(str(x).split()))\n","\n","# Group by label and calculate average length\n","avg_lengths = df.groupby('bias_label')['text_length'].mean()\n","\n","# Display the result\n","print(\"Average article length (in words) by label:\")\n","print(avg_lengths)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2V4dKFc3CbrC","executionInfo":{"status":"ok","timestamp":1748438218057,"user_tz":-120,"elapsed":2930,"user":{"displayName":"Johannes Stärk","userId":"02868779539657266620"}},"outputId":"372819cc-cd01-4eeb-b432-4b520dd84d1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["📏 Average article length (in words) by label:\n","bias_label\n","center    289.98156\n","left      373.33196\n","right     321.78388\n","Name: text_length, dtype: float64\n"]}]},{"cell_type":"markdown","source":["**To Delete**"],"metadata":{"id":"afO_H4UQwsOn"}},{"cell_type":"code","source":["# Load cleaned dataset\n","df = pd.read_parquet('/content/drive/MyDrive/NLP News Bias Data/df_cleaned_model_ready.parquet')\n","\n","X = df['text_clean']\n","y = df['bias_label']\n","\n","# Train/test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, stratify=y, test_size=0.2, random_state=42\n",")\n","\n","# Vectorizers\n","vectorizers = {\n","    'BoW': CountVectorizer(stop_words='english', max_features=20000),\n","    'TF-IDF': TfidfVectorizer(stop_words='english', max_features=20000)\n","}\n","\n","# Models\n","models = {\n","    'LogisticRegression': LogisticRegression(max_iter=500, solver='saga'),\n","    'NaiveBayes': MultinomialNB()\n","}\n","\n","# Run all combinations\n","for vec_name, vectorizer in vectorizers.items():\n","    print(f\"\\n Vectorizing with {vec_name}...\")\n","    X_train_vec = vectorizer.fit_transform(X_train)\n","    X_test_vec = vectorizer.transform(X_test)\n","\n","    for model_name, model in models.items():\n","        print(f\"\\n {model_name} + {vec_name}\")\n","        model.fit(X_train_vec, y_train)\n","        preds = model.predict(X_test_vec)\n","        acc = accuracy_score(y_test, preds)\n","\n","        print(f\" Accuracy: {acc:.4f}\")\n","        print(classification_report(y_test, preds))\n","\n","        # Confusion matrix\n","        cm = confusion_matrix(y_test, preds, labels=model.classes_)\n","        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n","        disp.plot(cmap='Blues', values_format='d')\n","        plt.title(f\"{model_name} + {vec_name} Confusion Matrix\")\n","        plt.show()"],"metadata":{"id":"yJHZD-r3cwLh","executionInfo":{"status":"aborted","timestamp":1748501996112,"user_tz":-120,"elapsed":621700,"user":{"displayName":"Johannes Stärk","userId":"02868779539657266620"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n","\n","# 📂 Load full dataset with only necessary columns\n","df = pd.read_parquet('/content/drive/MyDrive/NLP News Bias Data/data_newsbias_cleaned.parquet', columns=['text', 'bias_label', 'source'])\n","df = df.dropna(subset=['text', 'bias_label'])\n","\n","# ✅ Randomly sample 50k rows per class (balanced total = 150k)\n","df_sampled = (\n","    df.groupby('bias_label', group_keys=False)\n","      .apply(lambda g: g.sample(50000, random_state=42))\n","      .reset_index(drop=True)\n",")\n","\n","# 🧼 Define text cleaning function\n","def clean_for_model(text):\n","    text = text.lower()\n","    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n","    tokens = text.split()\n","    tokens = [t for t in tokens if t not in ENGLISH_STOP_WORDS]\n","    return ' '.join(tokens)\n","\n","# 🔁 Apply cleaning\n","df_sampled['text_clean'] = df_sampled['text'].astype(str).apply(clean_for_model)\n","\n","# 💾 Save cleaned, sampled dataset with source for later error analysis\n","df_sampled.to_parquet('/content/drive/MyDrive/NLP News Bias Data/df_sampled_cleaned.parquet', compression='snappy')\n","\n","print(f\"Preprocessing done. Shape: {df_sampled.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jo3VpKIo4shT","executionInfo":{"status":"ok","timestamp":1748435973222,"user_tz":-120,"elapsed":135994,"user":{"displayName":"Johannes Stärk","userId":"02868779539657266620"}},"outputId":"b3bf1a4e-8311-47da-acd3-7b48a13def92"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-1-42ca0ce4c14a>:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda g: g.sample(50000, random_state=42))\n"]},{"output_type":"stream","name":"stdout","text":["✅ Preprocessing done. Shape: (150000, 4)\n"]}]}]}